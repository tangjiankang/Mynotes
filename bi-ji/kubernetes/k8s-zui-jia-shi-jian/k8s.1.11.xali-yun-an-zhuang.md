# K8s.1.11.x阿里云安装HA版

#### 1.15后，集群安装方法有变动

#### 阿里云香港ubuntu16.04安装k8s 1.11.2 HA版

### **自动生成的证书只能用一年。\(有变动\)**

`openssl x509 -in apiserver-etcd-client.crt -text -noout`

## 架构说明

由于阿里云Ecs无法安装keepalived，我们采用阿里内部loadbalancer做master的负载，阿里内部loadbalancer无法由vpc内部ecs负载到自己机器，所以还需要另外两台服务器安装haproxy负载到三台master

```text
172.16.1.166    k8s-test-01  master,haproxy
172.16.1.167    k8s-test-02  master,haproxy
172.16.1.168    k8s-test-03  master,haproxy
172.16.1.169    k8s-test-04  node
172.16.1.170    k8s-test-api loadblancer ip
```

## 准备工作

* master节点做ssh打通
* 配置hosts解析

  ```text
  #master,k8s-test-api 指向本机通过 haproxy 负载
  cat >>/etc/hosts<<EOF
  172.16.1.166    k8s-test-01
  172.16.1.167    k8s-test-02
  172.16.1.168    k8s-test-03
  172.16.1.169    k8s-test-04
  127.0.0.1       k8s-test-api
  EOF
  # node,k8s-test-api 指向 loadblancer ip
  cat >>/etc/hosts<<EOF
  172.16.1.166    k8s-test-01
  172.16.1.167    k8s-test-02
  172.16.1.168    k8s-test-03
  172.16.1.169    k8s-test-04
  127.16.1.170    k8s-test-api
  EOF
  ```

  **安装docker**

  ```text
  # 卸载安装指定版本docker-ce
  yum remove -y docker-ce docker-ce-selinux container-selinux
  #出处https://docs.docker.com/install/linux/docker-ce/centos/#os-requirements
  #Centos 7
  yum install --setopt=obsoletes=0 \
   docker-ce-17.03.2.ce-1.el7.centos.x86_64 \
   docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch
  #ubuntu
  sudo apt-get update
  sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
  sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
  sudo apt-get -y update
  apt-get  -y install docker-ce=17.03.1~ce-0~ubuntu-xenial
  #
  systemctl enable docker
  systemctl start docker
  ```

  **配制haproxy**

  所有master都安装haproxy

  \`\`\`

  **拉取haproxy镜像**

  docker pull haproxy:1.7.8-alpine

  mkdir /etc/haproxy

  cat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOF

  global

  log 127.0.0.1 local0 err

  maxconn 50000

  uid 99

  gid 99

  **daemon**

  nbproc 1

  pidfile haproxy.pid

defaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2s

listen admin\_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy Statistics stats auth will:will stats hide-version stats admin if TRUE

frontend k8s-https bind 0.0.0.0:8443 mode tcp

## maxconn 50000

default\_backend k8s-https

backend k8s-https mode tcp balance roundrobin server lab1 172.16.1.166:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 172.16.1.167:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 172.16.1.168:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 EOF

## 启动haproxy

docker run -d --name my-haproxy  -v /etc/haproxy:/usr/local/etc/haproxy:ro  -p 8443:8443  -p 1080:1080  --restart always  haproxy:1.7.8-alpine

## 查看日志

docker logs my-haproxy

```text
# 安装 kubeadm, kubelet 和 kubectl
```

## centos7

## 配置源

cat &lt; /etc/yum.repos.d/kubernetes.repo \[kubernetes\] name=Kubernetes baseurl=[https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86\_64](https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64) enabled=1 gpgcheck=1 repo\_gpgcheck=1 gpgkey=[https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg](https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg) [https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg](https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg) EOF

## 安装

yum install -y kubelet kubeadm kubectl ipvsadm

## ubuntu 16.04

apt-get update && sudo apt-get install -y apt-transport-https curl -s [http://packages.faasx.com/google/apt/doc/apt-key.gpg](http://packages.faasx.com/google/apt/doc/apt-key.gpg) \| sudo apt-key add - cat &lt;/etc/apt/sources.list.d/kubernetes.list deb [http://mirrors.ustc.edu.cn/kubernetes/apt/](http://mirrors.ustc.edu.cn/kubernetes/apt/) kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl ipvsadm

```text
# 配置第一个master节点
```

## 生成配置文件

CP0\_IP="172.16.1.166" CP0\_HOSTNAME="k8s-test-01" cat &gt; kubeadm-config.yaml&lt;&lt;EOF apiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.11.2 apiServerCertSANs:

* k8s-test-01
* k8s-test-02
* k8s-test-03
* k8s-test-api
* 172.16.1.166
* 172.16.1.167
* 172.16.1.168
* 172.16.1.170
* 127.0.0.1

api: controlPlaneEndpoint: k8s-test-api:8443

etcd: local: extraArgs: listen-client-urls: "[https://127.0.0.1:2379,https://$CP0\_IP:2379](https://127.0.0.1:2379,https://$CP0_IP:2379)" advertise-client-urls: "[https://$CP0\_IP:2379](https://$CP0_IP:2379)" listen-peer-urls: "[https://$CP0\_IP:2380](https://$CP0_IP:2380)" initial-advertise-peer-urls: "[https://$CP0\_IP:2380](https://$CP0_IP:2380)" initial-cluster: "$CP0\_HOSTNAME=[https://$CP0\_IP:2380](https://$CP0_IP:2380)" serverCertSANs:

* $CP0\_HOSTNAME
* $CP0\_IP

  peerCertSANs:

* $CP0\_HOSTNAME
* $CP0\_IP

  controllerManagerExtraArgs:

  node-monitor-grace-period: 10s

  pod-eviction-timeout: 10s

  networking:

  **This CIDR is a Calico default. Substitute or remove for your CNI provider.**

  podSubnet: "192.168.0.0/16"  

  kubeProxy:

  config:

  mode: ipvs

  EOF

  **初始化**

  **注意保存返回的 join 命令**

  kubeadm init --config kubeadm-config.yaml

## 打包ca相关文件上传至其他master节点

cd /etc/kubernetes && tar cvzf k8s-key.tgz admin.conf pki/ca. _pki/sa._ pki/front-proxy-ca. _pki/etcd/ca._ scp k8s-key.tgz k8s-test-02:~/ scp k8s-key.tgz k8s-test-03:~/ ssh k8s-test-02 'tar xf k8s-key.tgz -C /etc/kubernetes/' ssh k8s-test-03 'tar xf k8s-key.tgz -C /etc/kubernetes/'

```text
# 配置第二个master
```

## 生成配置文件

CP0\_IP="172.16.1.166" CP0\_HOSTNAME="k8s-test-01" CP1\_IP="172.16.1.167" CP1\_HOSTNAME="k8s-test-02" cat &gt; kubeadm-config.yaml&lt;&lt;EOF apiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.11.2 apiServerCertSANs:

* k8s-test-01
* k8s-test-02
* k8s-test-03
* k8s-test-api
* 172.16.1.166
* 172.16.1.167
* 172.16.1.168
* 172.16.1.170
* 127.0.0.1

  api:

  controlPlaneEndpoint: k8s-test-api:8443

  etcd:

  local:

    extraArgs:

  ```text
  listen-client-urls: "https://127.0.0.1:2379,https://$CP1_IP:2379"
  advertise-client-urls: "https://$CP1_IP:2379"
  listen-peer-urls: "https://$CP1_IP:2380"
  initial-advertise-peer-urls: "https://$CP1_IP:2380"
  initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380"
  initial-cluster-state: existing
  ```

    serverCertSANs:

  * $CP1\_HOSTNAME
  * $CP1\_IP

    peerCertSANs:

  * $CP1\_HOSTNAME
  * $CP1\_IP

    controllerManagerExtraArgs:

    node-monitor-grace-period: 10s

    pod-eviction-timeout: 10s

    networking:

    **This CIDR is a Calico default. Substitute or remove for your CNI provider.**

    podSubnet: "192.168.0.0/16"  

    kubeProxy:

    config:

    mode: ipvs

    EOF

## 配置kubelet

kubeadm alpha phase certs all --config kubeadm-config.yaml kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml systemctl restart kubelet

## 添加etcd到集群中

CP0\_IP="172.16.1.166" CP0\_HOSTNAME="k8s-test-01" CP1\_IP="172.16.1.167" CP1\_HOSTNAME="k8s-test-02" KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0\_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=[https://${CP0\_IP}:2379](https://${CP0_IP}:2379) member add ${CP1\_HOSTNAME} [https://${CP1\_IP}:2380](https://${CP1_IP}:2380) kubeadm alpha phase etcd local --config kubeadm-config.yaml

## 部署

kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml kubeadm alpha phase controlplane all --config kubeadm-config.yaml kubeadm alpha phase mark-master --config kubeadm-config.yaml

```text
# 配制第三个master
```

## 生成配置文件

CP0\_IP="172.16.1.166" CP0\_HOSTNAME="k8s-test-01" CP1\_IP="172.16.1.167" CP1\_HOSTNAME="k8s-test-02" CP2\_IP="172.16.1.168" CP2\_HOSTNAME="k8s-test-03" cat &gt; kubeadm-config.yaml&lt;&lt;EOF apiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.11.2

apiServerCertSANs:

* k8s-test-01
* k8s-test-02
* k8s-test-03
* k8s-test-api
* 172.16.1.166
* 172.16.1.167
* 172.16.1.168
* 172.16.1.170
* 127.0.0.1

api: controlPlaneEndpoint: k8s-test-api:8443

etcd: local: extraArgs: listen-client-urls: "[https://127.0.0.1:2379,https://$CP2\_IP:2379](https://127.0.0.1:2379,https://$CP2_IP:2379)" advertise-client-urls: "[https://$CP2\_IP:2379](https://$CP2_IP:2379)" listen-peer-urls: "[https://$CP2\_IP:2380](https://$CP2_IP:2380)" initial-advertise-peer-urls: "[https://$CP2\_IP:2380](https://$CP2_IP:2380)" initial-cluster: "$CP0\_HOSTNAME=[https://$CP0\_IP:2380,$CP1\_HOSTNAME=https://$CP1\_IP:2380,$CP2\_HOSTNAME=https://$CP2\_IP:2380](https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380,$CP2_HOSTNAME=https://$CP2_IP:2380)" initial-cluster-state: existing serverCertSANs:

* $CP2\_HOSTNAME
* $CP2\_IP

  peerCertSANs:

* $CP2\_HOSTNAME
* $CP2\_IP

controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10s networking:

```text
# This CIDR is a Calico default. Substitute or remove for your CNI provider.
podSubnet: "192.168.0.0/16"
```

kubeProxy: config: mode: ipvs EOF

## 配置kubelet

kubeadm alpha phase certs all --config kubeadm-config.yaml kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml systemctl restart kubelet

## 添加etcd到集群中

CP0\_IP="172.16.1.166" CP0\_HOSTNAME="k8s-test-01" CP2\_IP="172.16.1.168" CP2\_HOSTNAME="k8s-test-03" KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0\_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=[https://${CP0\_IP}:2379](https://${CP0_IP}:2379) member add ${CP2\_HOSTNAME} [https://${CP2\_IP}:2380](https://${CP2_IP}:2380) kubeadm alpha phase etcd local --config kubeadm-config.yaml

## 部署

kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml kubeadm alpha phase controlplane all --config kubeadm-config.yaml kubeadm alpha phase mark-master --config kubeadm-config.yaml

```text
# 配置使用kubectl
```

rm -rf $HOME/.kube mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $\(id -u\):$\(id -g\) $HOME/.kube/config

## 配置实用网络插件

kubectl apply -f [https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml](https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml) kubectl apply -f [https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml](https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml)

## 查看node节点

kubectl get nodes

## 只有网络插件也安装配置完成之后，才能会显示为ready状态

## 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件

## 如 dashboard, heapster, efk等

kubectl taint nodes --all node-role.kubernetes.io/master-

```text
# 安装ingress-nginx
官网 https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md
```

## kubectl get po -n ingress-nginx

## kubectl logs -n ingress-nginx nginx-ingress-controller-6d466b5c9d-g9c62

已经用kubeadm自动部署了一套集群。 安装calico插件,再加入工作节点。 出处:[https://github.com/kubernetes/ingress-nginx/tree/master/deploy](https://github.com/kubernetes/ingress-nginx/tree/master/deploy)

## The following resources are required for a generic deployment.

## 集合rbac,default-backend,ingress一体

kubectl apply -f [https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml) kubectl apply -f [https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml)

## 验证default-backend

kubectl get service --all-namespaces ingress-nginx default-http-backend ClusterIP 10.107.250.86  80/TCP 2h curl 10.107.250.86 default backend - 404

```text
# 安装dashboard
```

kubectl apply -f [https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml](https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml) ----------------------------------------无用

mkdir /root/certs cd /root/certs openssl genrsa -des3 -out server.key 1024 openssl req -new -key server.key -out server.csr mv server.key server.key.org openssl rsa -in server.key.org -out server.key openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt

## 重建Secret kubernetes-dashboard-certs,如果之前装过dashboard.

kubectl delete secrets -n kube-system kubernetes-dashboard-certs

## 导入证书

kubectl create secret generic kubernetes-dashboard-certs --from-file=/root/certs -n kube-system ------------------------无用

## 创建管理用户 dashboard需要访问集群的权限，所以使用了clusterrolebinding

cat &gt; admin-user.yaml&lt;&lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: admin-user

### namespace: kube-system

apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects:

* kind: ServiceAccount

  name: admin-user

  namespace: kube-system

  EOF

  kubectl apply -f admin-user.yaml

  **配置ingress**

  cat &gt; dashboard-ingress.yaml&lt;&lt;EOF

  apiVersion: extensions/v1beta1

  kind: Ingress

  metadata:

  name: dashboard-ingress

  namespace: kube-system

  annotations:

    kubernetes.io/ingress.class: "nginx"

    nginx.ingress.kubernetes.io/secure-backends: "true"

  spec:

  tls:

  * hosts:
    * k8s.dashboard.com

      secretName: kubernetes-dashboard-certs

      rules:
  * host: k8s.dashboard.com

    http:

      paths:

    * path:

      backend:

        serviceName: kubernetes-dashboard

        servicePort: 443

      EOF

      kubectl apply -f dashboard-ingress.yaml

      **获取token用于登录**

      kubectl -n kube-system describe secret $\(kubectl -n kube-system get secret \| grep admin-user \| awk '{print $1}'i\)

      **本地绑定hostdashboard所在宿主机IP测试，开放443端口**

      \`\`\`

